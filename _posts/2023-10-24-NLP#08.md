---
layout: single
title:  "NLP#08"
---

# 정제(Cleaning) & 정규화(Normalization)

    토큰화란 토큰을 각 목적에 따라 분류한 것을 의미한다
    그전에 정제 및 정규화 과정을 거쳐야만 한다.
    정제(Cleaning) : 가지고 있는 코퍼스의 노이즈 데이터를 제거한다.
    정규화(Nomalization) : 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만드는 작업

## 규칙에 기반한 표기가 다른 단어들의 통합

    같은 의미를 가지고 있어도 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을
    사용할 수 있다.
    Kr과 Korea는 같은 의미이지만 표기가 다른 예시이다.

## 대,소문자 통합

    기존 대문자의 경우는 대부분 첫 단어일 경우이다. 또는 동사의 의미와 명사의 의미를
    가진 단어를 명사의 의미로 사용할 때 이와같이 대문자로 표기한다.
    하지만 이를 통합하지 않으면 후에 소문자로 해당 단어를 찾고자 할 때 어려움을 겪을
    수 있다.
    하지만 단어를 찾고자 하는 것이 아닌 의미를 찾고자 할 때는 이와 같이
    대,소문자를 통합하는것을 옳지 않다. 이는 분류 시에 오분류가 발생할 수 있기
    때문이다.

## 불필요한 단어의 제거
    
    노이즈 데이터란 자연어가 아니면서 아무 의미도 가지지 않는 글자(특수 문자)를
    의미하기도 하지만, 분석의 목적에 맞지 않는 단어들 또한 노이즈 데이터라고 한다.
    노이즈 데이터를 제거하는 방법으로는 등장 빈도에 따른 제거, 길이가 짧은 단어
    제거가 있다.

1. 등장 빈도가 적은 단어
    모델 학습에 있어서 데이터 불균형은 오히려 좋지 못한 결과를 도출합니다.
    이와 같이 등장 빈도가 적은 단어는 오히려 분류시에 오류를 범할 수 있습니다.

2. 길이가 짧은 단어
    먼저 영어권에서는 짧은 단어 a, an, as와 같은 2~3글자로 이루어진 단어가 있습니다.
    이를 제거할 경우 큰 의미를 갖지 못하는 단어를 줄이는 효과를 얻을 수 있습니다.
    하지만 한국어권에서는 다릅니다. 한국어의 경우는 단어의 길이가 평균적으로
    2~3글자로 이루어져 있기 때문에 길이가 짧다고 하여 무조건적으로 제거하는것은
    옳지 못한 방향입니다.


```python
import re
text = "I was wondering if anyone out there could enlighten me on this car."

# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
```

     was wondering anyone out there could enlighten this car.
    

## 정규 표현식(Regular Expression)

    노이즈 데이터의 특징을 알 수 있다면 정규 표현식을 통해 이를 제거할 수 있다.
    반복되는 패턴이 존재한다면 정규 표현식이 이러한 것을 한번에 제거가 가능한것이다.
