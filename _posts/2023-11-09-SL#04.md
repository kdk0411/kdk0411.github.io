---
layout: single
title:  "SL#04"
---
# 스팸여부판단

## 데이터셋 정보 확인


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

file_path = 'spam.csv'
data = pd.read_csv(file_path)
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>
</div>




```python
data['target'].unique()
```




    array(['ham', 'spam'], dtype=object)



## 전처리 : 특수 기호 제거
    특수 기호는 노이즈가 되므로 제거하는 것이 좋다.
    Python의 String을 통해서 그 목록을 추출하여 제거합니다.


```python
import string
string.punctuation
```




    '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'




```python
sample_string = data['text'].loc[0]
sample_string
```




    'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'




```python
arr = []
for i in sample_string:
    if i not in string.punctuation:
        arr.append(i)
print(arr)
```

    ['G', 'o', ' ', 'u', 'n', 't', 'i', 'l', ' ', 'j', 'u', 'r', 'o', 'n', 'g', ' ', 'p', 'o', 'i', 'n', 't', ' ', 'c', 'r', 'a', 'z', 'y', ' ', 'A', 'v', 'a', 'i', 'l', 'a', 'b', 'l', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 'b', 'u', 'g', 'i', 's', ' ', 'n', ' ', 'g', 'r', 'e', 'a', 't', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'l', 'a', ' ', 'e', ' ', 'b', 'u', 'f', 'f', 'e', 't', ' ', 'C', 'i', 'n', 'e', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'g', 'o', 't', ' ', 'a', 'm', 'o', 'r', 'e', ' ', 'w', 'a', 't']
    


```python
arr = ''.join(arr)
```


```python
def remove_punc(x):
    arr = []
    for i in sample_string:
        if i not in string.punctuation:
            arr.append(i)
    arr = ''.join(arr)
    return arr
```


```python
remove_punc(sample_string)
```




    'Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat'




```python
remove_punc(data['text'])
```




    'Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat'




```python
data['text'].apply(remove_punc)
```




    0       Go until jurong point crazy Available only in ...
    1       Go until jurong point crazy Available only in ...
    2       Go until jurong point crazy Available only in ...
    3       Go until jurong point crazy Available only in ...
    4       Go until jurong point crazy Available only in ...
                                  ...                        
    5569    Go until jurong point crazy Available only in ...
    5570    Go until jurong point crazy Available only in ...
    5571    Go until jurong point crazy Available only in ...
    5572    Go until jurong point crazy Available only in ...
    5573    Go until jurong point crazy Available only in ...
    Name: text, Length: 5574, dtype: object




```python
data['text'] = data['text'].apply(remove_punc)
```

## 전처리 : 불용어 제거
    불용어란 자연어 처리에 있어서 불필요한 단어라고 생각하면 된다.
    잔연어 처리에서는 각 단어가 하나의 독립변수처럼 작용하기 때문에 분석 과정에서
    데이터를 방대하게 펼치게 된다.
    이때는 nltk에서 제공하는 불용어 목록을 사용한다.
    이전 처럼 not in을 이용하여 목록에 존재하지 않는 단어만을 뽑아서 사용하면 된다.


```python
import nltk
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to
    [nltk_data]     C:\Users\Administrator\AppData\Roaming\nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    




    True




```python
from nltk.corpus import stopwords
stopwords.words('english')
```




    ['i',
     'me',
     'my',
     'myself',
     'we',
     'our',
     'ours',
     'ourselves',
     'you',
     "you're",
     "you've",
     "you'll",
     "you'd",
     'your',
     'yours',
     'yourself',
     'yourselves',
     'he',
     'him',
     'his',
     'himself',
     'she',
     "she's",
     'her',
     'hers',
     'herself',
     'it',
     "it's",
     'its',
     'itself',
     'they',
     'them',
     'their',
     'theirs',
     'themselves',
     'what',
     'which',
     'who',
     'whom',
     'this',
     'that',
     "that'll",
     'these',
     'those',
     'am',
     'is',
     'are',
     'was',
     'were',
     'be',
     'been',
     'being',
     'have',
     'has',
     'had',
     'having',
     'do',
     'does',
     'did',
     'doing',
     'a',
     'an',
     'the',
     'and',
     'but',
     'if',
     'or',
     'because',
     'as',
     'until',
     'while',
     'of',
     'at',
     'by',
     'for',
     'with',
     'about',
     'against',
     'between',
     'into',
     'through',
     'during',
     'before',
     'after',
     'above',
     'below',
     'to',
     'from',
     'up',
     'down',
     'in',
     'out',
     'on',
     'off',
     'over',
     'under',
     'again',
     'further',
     'then',
     'once',
     'here',
     'there',
     'when',
     'where',
     'why',
     'how',
     'all',
     'any',
     'both',
     'each',
     'few',
     'more',
     'most',
     'other',
     'some',
     'such',
     'no',
     'nor',
     'not',
     'only',
     'own',
     'same',
     'so',
     'than',
     'too',
     'very',
     's',
     't',
     'can',
     'will',
     'just',
     'don',
     "don't",
     'should',
     "should've",
     'now',
     'd',
     'll',
     'm',
     'o',
     're',
     've',
     'y',
     'ain',
     'aren',
     "aren't",
     'couldn',
     "couldn't",
     'didn',
     "didn't",
     'doesn',
     "doesn't",
     'hadn',
     "hadn't",
     'hasn',
     "hasn't",
     'haven',
     "haven't",
     'isn',
     "isn't",
     'ma',
     'mightn',
     "mightn't",
     'mustn',
     "mustn't",
     'needn',
     "needn't",
     'shan',
     "shan't",
     'shouldn',
     "shouldn't",
     'wasn',
     "wasn't",
     'weren',
     "weren't",
     'won',
     "won't",
     'wouldn',
     "wouldn't"]




```python
sample_string = data['text'].loc[0]
sample_string.split()
```




    ['Go',
     'until',
     'jurong',
     'point',
     'crazy',
     'Available',
     'only',
     'in',
     'bugis',
     'n',
     'great',
     'world',
     'la',
     'e',
     'buffet',
     'Cine',
     'there',
     'got',
     'amore',
     'wat']




```python
def stop_words(x):
    arr = []
    for i in sample_string.split():
        if i.lower() not in stopwords.words:
            arr.append(i.lower())
    arr = ' '.join(arr)
    return arr

data['text'] = data['text'].apply(stop_words)
data['text']
```

    go
    jurong
    point
    crazy
    available
    bugis
    n
    great
    world
    la
    e
    buffet
    cine
    got
    amore
    wat
    

## 전처리 : 목표 컬럼 형태 변경
    map을 이용하여 숫자로 변환 하는것이 가장 좋다고 생각한다.
    이에 맞춰 0,1로 spam과 ham을 변환해 줄 것이다.


```python
data['target'] = data['target'].map({'spam': 1, 'ham': 0})
data['target']
```




    0       0
    1       0
    2       1
    3       0
    4       0
           ..
    5569    1
    5570    0
    5571    0
    5572    0
    5573    0
    Name: target, Length: 5574, dtype: int64




```python
pip install scikit-learn
```

    Requirement already satisfied: scikit-learn in c:\users\administrator\anaconda3\lib\site-packages (1.3.0)
    Requirement already satisfied: numpy>=1.17.3 in c:\users\administrator\anaconda3\lib\site-packages (from scikit-learn) (1.24.3)
    Requirement already satisfied: scipy>=1.5.0 in c:\users\administrator\anaconda3\lib\site-packages (from scikit-learn) (1.11.3)
    Requirement already satisfied: joblib>=1.1.1 in c:\users\administrator\anaconda3\lib\site-packages (from scikit-learn) (1.1.1)
    Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\administrator\anaconda3\lib\site-packages (from scikit-learn) (2.2.0)
    Note: you may need to restart the kernel to use updated packages.
    

## 전처리 : 카운트 기반으로 벡터화
    문자를 개수 개반으로 벡터화 하는 방식을 사용한다.
    데이터셋에 존재하는 모든 단어에 각각 인덱스를 부여해주는것이다.
    이후 이 인덱스를 카운트하는 것을 의미한다.


```python
from sklearn.feature.extraction.text import CountVectorizer

x = data['text']
y = data['target']

cv = CountVectorizer()
cv.fit(x)
cv.vocabulary
```


    ---------------------------------------------------------------------------

    ModuleNotFoundError                       Traceback (most recent call last)

    Cell In[34], line 1
    ----> 1 from sklearn.feature.extraction.text import CountVectorizer
          3 x = data['text']
          4 y = data['target']
    

    ModuleNotFoundError: No module named 'sklearn.feature'



```python
x = cv.transform(x)
print(x)
```


```python
data.loc[0]['text']
```

## 모델링 및 예측/평가
    다항 분포에 대한 Naive Bayes알고리즘을 이용한다.
    이에는 다항 분포(Multinormaial), 정규분포(Gaussian), 베르누이 분포(Bernoulli)가 존재하며 상황에 따라 적용하면된다.


```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_text_split(x, y, test_size=0.2, random_state=100)
```


```python
from sklearn.naive_bayse import MultinomiaLNB
model = MultinomiaLNB()
model.fit(x_train, y_train)
pred = model.predict(x_test)
```


```python
from sklearn.metrics import accuracy_score, confusion_matrix
accuracy_score(y_test, pred)
print(confusion_matrix(y_test, pred))
```


```python
sns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.0f')
```
