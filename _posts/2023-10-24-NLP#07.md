# 토큰화
    자연어 처리에서 크롤링 등으로 수집한 코퍼스(말뭉치) 데이터가 필요에 맞게 전처리 되어있지
    않은 상태라면, 해당 데이터를 사용하고자 하는 용도에 맞게 토큰화&정제&정규화를 하게 됩니다.
    그중 토큰화에 대해 알아보겟습니다.

    주어진 코퍼스(말뭉치)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화라고 합니다.
    토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의합니다.
    NLTK, KoNLPY를 이용하여 실습을 해보자.

## 단어 토큰화(Word Tokenization)
    토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화라고 합니다.
    여기서 단어는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 합니다.

    Input : Time is an illusion. Lunchtime double so!
    라는 입력을 구두점을 제외시킨 토큰화 작업을 거치면 다음과 같은 결과가 도출됩니다.
    Output : "Time", "is", "an", "illusion", "Lunchtime", "double", "so"
    이 토큰화 작업은 굉장히 간단한 예시입니다.
    구두점을 지운 뒤 띄어쓰기를 기준으로 나눈것입니다.

    보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제 작업을 수행하는
    것만으로 해결되지 않습니다. 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어
    버리는 경우가 발생합니다. 믈론 띄워쓰기를 기준으로 나누게 되면 사실상 단어 토큰이
    구분되는 영어와 달리 한국어는 구분이 어렵습니다.

## 토큰화 중 생기는 선택의 순간
    많은 경우 중 (')가 들어간 단어는 어떻게 토큰화 해야할지 고민을 하게 됩니다.
    이 경우를 한번 알아보겠습니다.
    아래와 같은 문자을 예시로 해보겟습니다.
    Don't be fooled by the dark sounding name, Mr. Jone's Orphanage
    is as cheery as cheery goes for a pastry shop.

    Don't는 여러 가지의 선택지가 있습니다.
    1. Don't
    2. Don t
    3. Dont
    4. Do n't
    5. Jone's
    6. Jone s
    7. Jone
    8. Jones
    이 중 사용자가 원하는 결과가 나오도록 토큰화 도구를 커스터마이징 할 수 있지만
    기존에 공개된 도구들을 사용하였을 때의 결과가 사용자의 목적과 일치한다면 해당
    도구를 사용할 수 도있습니다.
    NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공합니다.
    그중 word_tokenize와 WordPunctTokenizer를 사용하여 (')를 어떻게 처리하는지
    확인해보겟습니다.

    먼저  word_tokenize를 사용해보겠습니다.


```python
! pip install tensorflow
```


```python
import nltk
nltk.download()
```

    showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml
    




    True




```python
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
```


```python
print('단어 토큰화1 :',word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화1 : ['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
    

-> word_tokenize는 Don't를 Do와 n't로 분리했지만 Jone's는 Jone과 's로 분리한것을 알 수 있습니다.

    그럼 두 번째로 WordPunctTokenizer는 어떻게 처리하는지 알아보겠습니다.


```python
print('단어 토큰화2 :',
      WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화2 : ['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
    

-> WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 가지고 있기 때문에 word_tokenize와는 달리
    Don't를 Don과 '와 t로 분리 하였으며 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리한것을 알 수 있습니다.

    케라스(keras)또한 토큰화 도구로 text_to_word_sequence를 지원합니다.


```python
print('단어 토큰화3 :',text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화3 : ["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
    

-> 케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표(.)나 컴마(,), 느낌표(!)등의
    구두점을 제거합니다. 하지만 don't나 jone'와 같은 경우 (')는 보존하는것을 알 수 잇습니다.

# 토큰화에서 고려해야할 사항
    토큰화 작업은 단순하게 구두점을 제외하고 공백을 기준으로 나누는 작업이 아니다.

1. 구두점이나 특수 문자를 단순 제외해서는 안된다.
    단순하게 특수 문자를 기준으로 코퍼스를 나누는 것은 잘못된 방법이다.
    단어를 강조하는데 마침표(.)를 사용하기도 하는데 이를 문장의 끝으로
    이해하고 나누는것은 잘못된 것이다.
    또한 슬래시(/)나 컴마(,)의 경우도 날짜 2023/10/23, 숫자의 수치를
    표현하는 135,79와 같은 경우도 존재하기 때문에 막무가내로 특수문자를
    기준으로 잡고 나누는 것은 안된다는 것이다.

2. 줄임말과 단어 내에 띄어쓰기가 있는 경우
    하나의 단어이지만 문자 사이에 띄어쓰기가 포함되어있는 경우가 존재한다.
    이러한 경우를 생각하였을때도 띄어쓰기가 절대적인 구분점이 아니라는것을
    알아야 한다.

3. 표준 토큰화 예제
    Penn Treebank Tokenization을 예시로 코드를 한번 살펴보자.
    여기에는 2가지의 규칙이 있기때문에 유의해야 한다.
    
    규칙 1. 하이픈(-)으로 구성된 단어는 하나로 유지
    규칙 2. doesn't와 같이 (')로 '접어'가 포함되는 단어는 분리


```python
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))
```

    트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', "n't", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']
    

-> 결과를 보면 규칙1과 규칙2에 따라 home-based는 하나의 토큰으로 취급한다.
    dosen't의 경우 does와 n't는 분리된것을 알 수 있다.

# 문장 토큰화(Sentence Tokenization)
    토큰의 단위가 항상 단어일 필요는 없다.
    이번에는 토큰의 단위가 문장인 경우를 보자
    이를 문장 분류(Sentence Segmentation)이라고도 부른다.
    아래 NLTK에서는 영어 문자 토큰화를 할 수 있는 sent_tokenize가 존재한다.
    코드로 확인해보자


```python
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print('문장 토큰화2 :',sent_tokenize(text))
```

    문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']
    문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
    

-> NLTK는 단순하게 특수문자로 구분하지 않기 때문에 문장을 정상적으로
    인식한것을 볼 수 있다.

한국어의 경우는 KSS를 사용할 수 있다.


```python
pip install kss
```


```python
# KSS를 사용하여 문장 토큰화를 해보자
import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))
# 이와 같이 진행하면 정상적으로 문장이 분리된것을 확인할 수 있다.
```

    [Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D
    For your information, Kss also supports mecab backend.
    We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.
    Please refer to following web sites for details:
    - mecab: https://cleancode-ws.tistory.com/97
    - konlpy.tag.Mecab: https://uwgdqo.tistory.com/363
    
    

    한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']
    

# 한국어에서의 토큰화의 어려움
    한국어는 영어에 비해 토큰화 작업이 어려운 편이다. 영어는 줄임말, 띄어쓰기만을
    신경쓰기만 하여도 충분히 토큰화가 진행되는 반면 한국어는 그렇지 않기 때문이다.

1. 교착어
    한국어에는 '조사'라는 것이 있다. 일반적으로 '은', '는', '이', '가' 가 존재한다.
    이는 앞 글자에 띄어쓰기 없이 붙는다. 때문에 이를 모두 분리할 필요가 있다.
    또한 한국어 토큰화에는 '형태소'의 개념이 중요하다.
    '형태소'는 뜻을 가진 가장 작은 말의 단위를 의미한다. 이에는 2가지의 형태가 있다.

- 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
                그 자체로 단어가 된다는 특징이 있다.
                체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.

- 의존 형태소 : 다른 형태소와 결합하여 사용된느 형태소
                접사, 어미, 조사, 어간을 의미한다.

2. 한국어의 띄어쓰기
    한국어는 영어와 다르게 띄어쓰기가 단어마다 이루어지지 않는다.
    예시로 "할 수 있다."와 "할 수도 있다."가 있다.
    이 두 문장은 "수"의 영향으로 띄어쓰기가 진행되지만 "도"의 경우는 "수"에 붙는
    모습을 볼 수 있다.

# 품사 태깅(Part-of-speech tagging)
    동음이어와 같이 같은 말이지만 그 뜻이 다르거나 품사에 따라 의미가 바뀌는
    경우가 존재한다. 따라서 단어의 품사를 파악하여 단어의 쓰임을 알아내는것이
    중요하다고 할 수 있다. 이처럼 각 단어의 품사를 구분하는 작업을
    품사 태깅(Part-of-speech tagging)이라고 한다.

# NLTK와 KoNLPY를 이용한 영어, 한국어 토큰화 실습
    NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅합니다.


```python
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('단어 토큰화 :',tokenized_sentence)
print('품사 태깅 :',pos_tag(tokenized_sentence))
```

    단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']
    품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]
    

-> 영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다.
    Penn Treebank POS Tags에서 PRP는 인칭대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사,
    NNP는 고유명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.
